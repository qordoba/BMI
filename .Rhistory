s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
-3 - 1 + c(-1, 1) * qt(0.95, 198) * spsq * (1/100 + 1/100)^0.5
n1 <- n2 <- 100
x1 <- 4  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
6- 4 + c(-1, 1) * qt(0.95, 198) * spsq * (1/100 + 1/100)^0.5
n1 <- n2 <- 100
x1 <- 1  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
6- 4 + c(-1, 1) * qt(0.95, 198) * spsq * (1/100 + 1/100)^0.5
n1 <- n2 <- 100
x1 <- 1  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
x2- x1 + c(-1, 1) * qt(0.95, 198) * spsq * (1/100 + 1/100)^0.5
n1 <- n2 <- 10
x1 <- 3  ##treated
x2 <- 5  ##placebo
s1 <- 0.6  ##treated
s2 <- 0.68  ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
x1 -x2 + c(-1, 1) * qt(0.975, 18) * spsq * (1/10 + 1/10)^0.5
n1 <- n2 <- 10
x1 <- 3  ##treated
x2 <- 5  ##placebo
s1 <- 0.6  ##treated
s2 <- 0.68  ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
x1 -x2 + c(-1, 1) * qt(0.95, 18) * spsq * (1/10 + 1/10)^0.5
n1 <- n2 <- 10
x1 <- 3  ##treated
x2 <- 5  ##placebo
s1 <- 0.6  ##treated
s2 <- 0.68  ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
md <- x1 -x2
semd <- spsq * sqrt(1/n1 + 1/n2)
md + c(-1, 1) * qt(0.975, n1 + n2 - 2) * semd
n1 <- n2 <- 100
x1 <- 4  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
6- 4 + c(-1, 1) * qt(0.95, 198) * spsq * (1/n1 + 1/n2)^0.5
n1 <- n2 <- 100
x1 <- 4  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
x2- x1 + c(-1, 1) * qt(0.95, 198) * spsq * (1/n1 + 1/n2)^0.5
n1 <- n2 <- 100
x1 <- 1  ##treated
x2 <- 6  ##placebo
s1 <- 0.5  ##treated
s2 <- 2 ##placebo
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
x2- x1 + c(-1, 1) * qt(0.95, 198) * spsq * (1/n1 + 1/n2)^0.5
install.packages("UsingR")
library('UsingR')
library(UsingR); data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
plot(galton$parent,galton$child,pch=19,col="blue")
pt(0.8, 15)
pt(0.8, 15, lowe.tail=FALSE)
pt(0.8, 15, lower.tail=FALSE)
qt(0.95,99)
1.66*1.1/10
3-0.182
qt(0.975,8)
round(pbinom(2,prob=.75, size=4, lower.tail=FALSE)
round(pbinom(2,prob=.75, size=4, lower.tail=FALSE))
round(pbinom(2,prob=.5, size=4, lower.tail=FALSE))
m1 <- -3; m2 <- 1
n1 <- n2 <- 9
s1 <- 1.5
s2 <- 1.8
spsq <- sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))
ts <- (m2 - m1) / spsq
pv <- 2 * pnorm(-abs(ts))
qnorm(.95) + qnorm(.9)) ^ 2 * .04 ^ 2 / .01^2
(qnorm(.95) + qnorm(.9)) ^ 2 * .04 ^ 2 / .01^2
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot"))
p1
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(Hmisc)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot"))
p1
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
cutWage <- cut2(training$wage,g=3)
wgae
wage
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
cutWage <- cut2(training$wage,g=3)
qplot(wage,colour=education,data=training,geom="density")
qplot(wage,colour=education,data=training,geom="density")
wage
qplot(Wage,colour=education,data=training,geom="density")
cutWage <- cut2(training$Wage,g=3)
library(ISLR); library(ggplot2); library(caret);
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
cutWage <- cut2(training$wage,g=3)
table(cutWage)
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot"))
p1
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
qplot(wage,colour=education,data=training,geom="density")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
View(predictors)
adData =data.frame(predictors)
trainIndex = createDataPartition(diagnosis, p=0.5, list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
plot(training$CompressiveStrength, inTrain)
plot(inTrain, training$CompressiveStrength)
cutCS <- cut2(training$CompressiveStrength,g=3)
p1 <- qplot(cutCS,inTrain, data=training,fill=cutCS,
geom=c("boxplot"))
p1
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(training)
hist(SuperPlasticizer )
hist(training$SuperPlasticizer)
hist(as.numeric(training$SuperPlasticizer))
hist(as.numeric('training$SuperPlasticizer'))
training$Superplasticizer
log(training$Superplasticizer)
a <- log(training$Superplasticizer)
hist(a)
b <- training$Superplasticizer
hist(b)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
modelFit <- train(training$diagnosis ~ .,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$diagnosis,predict(modelFit,testing))
head(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
preProcess.default(x = training[, c(58:69)], method = "pca", thresh = 0.9)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
plot(training$CompressiveStrength)
head(training)
training$FlyAsh
training$Age
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e -(y - yhat)))
plot(x, y); abline(lm(y ~ x))
summary(fit)$sigma
library(UsingR)
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e -(y - yhat)))
plot(x, y); abline(lm(y ~ x))
summary(fit)$sigma
summary(fit)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)
data(mtcars)
head(mtcars)
x <- mtcars$wt
y <- mtcars$mpg
fit <- lm(y ~ x)
summary(fit)
hunger <- read.csv("hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
head(hunger)
data(mtcars)
library(ggplot2)
data(mpg)
head(mpg)
data*mtcars
data(mtcars)
head(mtcars)
a <- lm(mtcars$mpg ~ as.factor(mtcars$cyl))
summary*a
summary(a)
a <- lm(mtcars$mpg ~ as.factor(mtcars$cyl) + wt)
a <- lm(mtcars$mpg ~ as.factor(mtcars$cyl) + mtcars$wt)
summary(a)
b <- lm(mtcars$mpg ~ as.factor(mtcars$cyl))
summary(b)
a <- lm(mtcars$mpg ~ as.factor(mtcars$cyl) + mtcars$wt -1)
summary(a)
b <- lm(mtcars$mpg ~ as.factor(mtcars$cyl) -1)
summary(b)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm (y ~ x)
hats<-lm.influence(fit)$hat
hats
dfbeta(fit)
dfbetas(fit)
summary(lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars))
head(mtcars)
data <- data(mtcars)
boxplot(data$mpg ~ data$am, main = "MPG by transmission", xlab = "Transmission type", ylab = "Miles per gallon")
head(mtcars)
data <- data(mtcars)
levels(data$am) <- c("Auto", "Manual")
data(mtcars)
head(mtcars)
View(mtcars)
data(mtcars)
levels(mtcars$am) <- c("Auto", "Manual")
boxplot(mpg ~ am, data=mtcars, main = "MPG by transmission", xlab = "Transmission type", ylab = "Miles per gallon")
### Figure #2: Pairs figure
data(mtcars)
head(mtcars)
levels(mtcars$am) <- c("Auto", "Manual")
head(mtcars)
data(mtcars)
a <- mtcars
levels(a$am) <- c("Auto", "Manual")
head(a)
data(mtcars)
mtcars$am <- factor(mtcars$am)
levels(mtcars$am) <- c("Auto", "Manual")
View(mtcars)
library(MASS)
data(mtcars)
mtcars$am <- factor(mtcars$am) ## for simpler calculations we will factor it then level it
levels(mtcars$am) <- c("Auto", "Manual")
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
fit_all <- lm(mpg ~ ., data = mtcars)
step <- stepAIC(fit_all, direction="both")
step$anova
library(ElemStatLearn)
train <- data(vowel.train)
test <- data(vowel.test)
library(ElemStatLearn)
train <- data(vowel.train)
test <- data(vowel.test)
modFitrf <- train(y~ .,data=vowel.train,method="rf",prox=TRUE)
library(caret)
modFitrf <- train(y~ .,data=vowel.train,method="rf",prox=TRUE)
mod1 <- train(y ~.,method="glm",data=vowel.train)
mod2 <- train(y ~.,method="rf",
data=vowel.train,
trControl = trainControl(method="cv"),number=3)
mod1$finalModel
mod2$finalModel
mod1 <- train(y ~.,method="gbm",data=vowel.train)
set.seed(33833)
mod2 <- train(y ~.,method="gbm",data=vowel.train)
mod1 <- train(y ~.,method="rf",
data=vowel.train,
trControl = trainControl(method="cv"),number=3)
mod1$finalModel
mod1$finalModel
mod2$finalModel
pred1 <- predict(mod1,vowel.test)
pred2 <- predict(mod2,vowel.test)
confusionMatrix(pred1,vowel.test$y)
library(caret);
library(ElemStatLearn);
data(vowel.train);
data(vowel.test);
#Set seed as instructed in the quiz question.
set.seed(33833);
#Convert the y values to factors.
vowel.train$y = factor(vowel.train$y);
vowel.test$y = factor(vowel.test$y);
#Train the classifiers.
train_model_RF = train(y ~ ., method="rf", data = vowel.train);
train_model_GBM = train(y ~ ., method="gbm", data = vowel.train);
#Use the models to predict.
predict_RF = predict(train_model_RF, vowel.test);
predict_GBM = predict(train_model_GBM, vowel.test);
accuracy_GBM = (vowel.test$y == predict_GBM);
accuracy_RF = (vowel.test$y == predict_RF);
length(accuracy_GBM[accuracy_GBM == TRUE]);
length(accuracy_RF[accuracy_RF == TRUE]);
length(accuracy_RF[accuracy_RF == TRUE]) / length(accuracy_RF);
length(accuracy_GBM[accuracy_GBM == TRUE]) / length(accuracy_GBM);
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
train_model_RF = train(diagnosis ~ ., method="rf", data = training);
train_model_GBM = train(diagnosis ~ ., method="gbm", data = training);
train_model_LDA = train(diagnosis ~ ., method="lda", data = training);
train_model_RF$finalmodel
train_model_RF$finalModel
train_model_GBM$finalModel
train_model_LDA$finalModel
predict_RF = predict(train_model_RF, testing);
predict_GBM = predict(train_model_GBM, testing);
predict_LDA = predict(train_model_LDA, testing);
confusionMatrix(predict_LDA,testing$diagnosis)
confusionMatrix(predict_GBM,testing$diagnosis)
confusionMatrix(predict_RF,testing$diagnosis)
df = data.frame(predict_RF, predict_GBM, predict_LDA, testing$diagnosis);
train_stacked = train(testing.diagnosis ~ ., method="rf", data = df);
predict_stacked = predict(train_stacked, df);
confusionMatrix(predict_stacked,testing$diagnosis)
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date)  2011,]
tstrain = ts(training$visitsTumblr)
dat.fit <- bats(training$visitsTumblr)
pred <- forecast(dat.fit, h=length(testing$visitsTumblr),level=c(80,95))
max(pred$lower[,2])
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
dat = read.csv("gaData.csv")
training = dat[year(dat$date)  2011,]
tstrain = ts(training$visitsTumblr)
training = dat[year(dat$date)  2011,]
training = dat[year(dat$date) 2011,]
training = dat[year(dat$date)==2011,]
training = dat[year(dat$date)==2011,]
tstrain = ts(training$visitsTumblr)
training = dat[year(dat$date)==2011,]
View(dat)
library(lubridate)
install.packages('lubridate')
library(lubridate)
training = dat[year(dat$date)==2011,]
tstrain = ts(training$visitsTumblr)
dat.fit <- bats(training$visitsTumblr)
pred <- forecast(dat.fit, h=length(testing$visitsTumblr),level=c(80,95))
max(pred$lower[,2])
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
install.packages('forecast')
library(forecast)
dat.fit <- bats(training$visitsTumblr)
pred <- forecast(dat.fit, h=length(testing$visitsTumblr),level=c(80,95))
max(pred$lower[,2])
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
dat.fit <- bats(training$visitsTumblr)
testing = dat[year(dat$date)>2011,]
pred <- forecast(dat.fit, h=length(testing$visitsTumblr),level=c(80,95))
max(pred$lower[,2])
accuracy <- 1-sum(testing$visitsTumblr>pred$upper[,2])/length(testing$visitsTumblr)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm.model <- svm(CompressiveStrength ~ ., data = training)
predict_mod  <- predict(svm.model, testing[,-9])
(sum((testing$CompressiveStrength - predict_mod)^2)/length(predict_mod))^0.5
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots <- 0;
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
summary(lm(y ~ xMat-1))
library(MASS)
fit <- glm(use ~ factor(wind) + magn, data=shuttle, family="binomial")
summary(fit)
exp(fit$coeff)
install.packages("shiny")
library(shiny)
runApp(C:\Users\Seif\Dropbox\data analysis\code\webapp)
runApp("C:\Users\Seif\Dropbox\data analysis\code\webapp")
runApp("C:\Users\Seif\Dropbox\data analysis\code\webapp\")
runApp("C:\\Users\\Seif\\Dropbox\\data analysis\\code\\webapp\\")
runApp("C:\\Users\\Seif\\Dropbox\\data analysis\\code\\webapp")
ls
clear
clc
clear all
install.packages("shinyapps")
runApp()
library(shiny)
runApp()
setwd("~/Data Analysis/Code/webapp")
pwd
pwd()
getwd()
runApp()
deployApp()
install.packages('shinyapps')
install.packages('shinyapps')
devtools::install_github('rstudio/shinyapps')
library(shinyapps)
getwd()
deployApp()
shinyapps::setAccountInfo(name='qordoba', token='99679AF3E0D67D539B90B7900460BC41', secret='2w0kqpN4kC90VbM0dKPcZdpchKglExjUVS5XqQ0T')
shinyapps::setAccountInfo(name='qordoba', token='99679AF3E0D67D539B90B7900460BC41', secret='2w0kqpN4kC90VbM0dKPcZdpchKglExjUVS5XqQ0T')
deployApp()
install_github('slidify', 'ramnathv')
devtools:: install_github('slidify', 'ramnathv')
devtools:: install_github('slidifyLibraries', 'ramnathv')
getwd()
setwd('~/Dropbox/Data Analysis/Code/datadev')
setwd("~/Data Analysis/Code/datadev/Pitch")
author("prediction")
;ibrary(slidofy)
library("slidify")
author("prediction")
weight <- 4
height <- 2
weight / (height^2)
setwd("C:/Users/Seif/Documents/GitHub/BMI")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
